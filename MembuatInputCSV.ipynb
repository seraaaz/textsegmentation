{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede442d0-a0a2-451c-87e7-73f1449616d9",
   "metadata": {},
   "source": [
    "### Membuat file input1.csv\n",
    "\n",
    "`input1.csv` memiliki 2 kolom, `doc_id` sebagai nomor tiap teks dokumen dan `doc_text` sebagai rawtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b397243-5ab6-4588-bec4-8be6c1bb5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel(\"./Split Anotasi Data 2-2.xlsx\", sheet_name=\"Data Awal\")\n",
    "df = pd.DataFrame(dataset, columns = [\"No\", \"Answer \"])\n",
    "df.columns = [\"doc_id\", \"doc_text\"]\n",
    "df.to_csv(\"input1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e590d-1e07-445f-a5be-658e0faff6fc",
   "metadata": {},
   "source": [
    "### Membuat file input2.csv\n",
    "\n",
    "`input2.csv` memiliki 3 kolom, `doc_id` sebagai nomor tiap teks dokumen, `sentence_id` sebagai nomor tiap kalimat dari teks terkait, dan `sentence_text` sebagai kalimat. Di sini, terdapat 2 tahap pemrosesan. Pertama dengan `textsplit`, kedua dengan `pysbd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554b9a2e-5ef8-4eba-bf76-6e7cf4b75692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textsplit.tools import SimpleSentenceTokenizer\n",
    "from pysbd import Segmenter\n",
    "\n",
    "sentence_tokenizer = SimpleSentenceTokenizer()\n",
    "seg = Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "out = {\"doc_id\": [], \"sentence_id\": [], \"sentence_text\": []}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    splitted = [s.strip() for s in sentence_tokenizer(row[\"doc_text\"])]\n",
    "    sent = \" \".join(splitted)\n",
    "    tokenized_text = [s for s in seg.segment(sent) if s not in (\". \", \".\", \"..\", \"...\", \"....\", \".....\")]\n",
    "    \n",
    "    for sentence_id, sentence_text in enumerate(list(tokenized_text)):\n",
    "        out[\"doc_id\"].append(row[\"doc_id\"])\n",
    "        out[\"sentence_id\"].append(f\"{row['doc_id']}_{sentence_id}\")\n",
    "        out[\"sentence_text\"].append(sentence_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79467180-ea70-4f14-a5d6-716cf39216ad",
   "metadata": {},
   "source": [
    "### Buat dataframe dari hasil tokenization dan export ke CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4113251a-87c7-4a19-a5f3-2d0db572c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(out)\n",
    "out_df.to_csv(\"input2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
