{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92588961-3e94-45c0-b01d-92d7ef505381",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Segmentasi Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503aef5-9ee2-49c4-9d25-fcb7103be4b9",
   "metadata": {},
   "source": [
    "## Impor library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352e83b4-1360-4323-88f3-5fd5019e92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gensim\n",
    "import word2vec\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7ebb1-2844-4432-9b66-d7c986355acb",
   "metadata": {},
   "source": [
    "## Load pre-trained model word2vec Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad5d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumber: https://github.com/deryrahman/word2vec-bahasa-indonesia\n",
    "model = Word2Vec.load(\"idwiki_word2vec_100.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed258209-fcb7-46e7-8de5-834b61163189",
   "metadata": {},
   "source": [
    "## Data untuk segmentasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c27d3dc-4301-4168-8780-605742dca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Split Anotasi Data 2-2.xlsx\"\n",
    "csv_file = pd.read_excel(file_path, sheet_name=\"Data Awal\")\n",
    "\n",
    "answers = csv_file[\"Answer \"].to_list()\n",
    "sentence_example = answers[194]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74022802-f850-4808-9b76-f5e6412177e0",
   "metadata": {},
   "source": [
    "## Teks terlebih dahulu di-tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d52d82-5b15-4fed-8d6d-4feffe8441e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textsplit.tools import SimpleSentenceTokenizer\n",
    "sentence_tokenizer = SimpleSentenceTokenizer()\n",
    "\n",
    "sentenced_text = sentence_tokenizer(sentence_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c3673-13dc-492a-9180-3445711dd3dd",
   "metadata": {},
   "source": [
    "## Membangun wordvector dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8328dd7-69ea-4a48-aeff-13d3ac210fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46885454/how-to-create-a-dataframe-with-the-word2ve-vectors-as-data-and-the-terms-as-row\n",
    "\n",
    "ordered_vocab = [(v, model.wv.key_to_index[v], model.wv.get_vecattr(v, \"count\")) for v in model.wv.index_to_key]\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda k: k[2])\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "wordvec = pd.DataFrame(model.wv.vectors[term_indices, :], index=ordered_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ef5e5-cc11-416b-b2ac-eabb5bfe0598",
   "metadata": {},
   "source": [
    "## Membangun CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5b732e-e65a-49d0-ab83-c30def76e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(vocabulary=wordvec.index)\n",
    "\n",
    "sentence_vec = count_vec.transform(sentenced_text).dot(wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121133b-cffa-495a-998a-e403f65ac79c",
   "metadata": {},
   "source": [
    "## Segmentasi menggunakan algoritma optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd6f350-2c32-4dbb-b812-8b78942c2c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 sentences, 7 segments, avg. 3 sentences per segment\n",
      "[[\"Halo   Bunda Rizkisusan, terima kasih  telah bertanya di Alodokter :')  \"\n",
      "  'Berdasarkan panduan WHO dan IDAI (Ikatan Dokter Anak Indonesia), pemberian '\n",
      "  'MPASI (Makanan Pendamping ASI) diberikan pada saat anak berusia 6 bulan. '],\n",
      " ['Hal ini disebabkan karena anak usia 6 bulan dinilai sudah memiliki saluran '\n",
      "  'pencernaan yang lebih kuat untuk dapat menerima makanan. ',\n",
      "  'Namun kendati demikian terdapat beberapa kondisi yang memperboleh seorang '\n",
      "  'anak mendapat MPASI dibawah usia 6 bulan yaitu jika anak dinilai tidak '\n",
      "  'memiliki berat badan yang ideal, atau anak sulit mengalami peningkatan '\n",
      "  'berat badan. '],\n",
      " ['Pemberian awal MPASI dapat dimulai dengan pemberian menu tunggal selama 1 - '\n",
      "  '2 minggu. ',\n",
      "  'Dalam satu hari anak sudah dapat mulai diajarkan untuk makan 2 - 3 kali '\n",
      "  'sehari dengan menu yang berbeda - beda, dengan komposisi makanan yang '\n",
      "  'mengandung karbohidrat - protein - sayuran serta buah - buahan sebagai '\n",
      "  'makanan selingan antara 2 waktu makan berat. '],\n",
      " ['Sebagi contoh : Hari I : pagi : Beras putih / kentang / ubi / kabocha / '\n",
      "  'butternut pumpkin / beras merah  / dan karbohidrat lainnya selingan I : air '\n",
      "  'perasan jeruk manis / buah - buahan lain siang : Tofu / tempe / edamame / '\n",
      "  'protein simple selingan II : pisang / buah - buahan lain sore : buncis / '\n",
      "  'wortel / sayuran lainnya. '],\n",
      " ['Untuk minggu pertama usahakan berikan makanan yang mengandung protein '\n",
      "  'simple. ',\n",
      "  'Kemudian diminggu kedua anda sudah dapat mulai memberikan protein kompleks '\n",
      "  'seperti protein hewani Untuk minggu ke 2 atau ke 3 dan selanjutnya anda '\n",
      "  'sudah dapat memberikan makanan yang mengandung lemak dengan menu bintang 2 '\n",
      "  'atau bintang 3 hingga bintang 4 dan bintang 5 . ',\n",
      "  'Yang dimaksud dengan menu bintang adalah menu campuran antara makanan '\n",
      "  'dengan komponen gizi yang berbeda. ',\n",
      "  'Sebagai contoh seperti : (beras putih + wortel + ayam). '],\n",
      " ['Tidak ada patokan waktu yang menjadi ketetapan pasti jadwal pemberian makan '\n",
      "  'anak. ',\n",
      "  'Yang terpenting saat disuapi, anak harus dalam kondisi yang senang dan '\n",
      "  'nyaman. ',\n",
      "  'Hindari pemberian makan disaat anak mengantuk, berikan makanan disaat anak '\n",
      "  'merasa segar dan nyaman. ',\n",
      "  'Biasakan anak makan dimeja atau dikursi makannya agar anak fokus saat '\n",
      "  'makan, dan dapat menikmati waktu makannya. ',\n",
      "  'Hindari pemberian makan bersamaan dengan menonton tv atau sambil digendong '\n",
      "  'dan berjalan - jalan main keluar rumah. ',\n",
      "  'Jika belum bisa duduk, anak tetap dapat didudukkan di kursi makannya '\n",
      "  'mungkin dapat dibantu dengan menggunakan bantal atau alas yang lembut agar '\n",
      "  'anak merasa lebih nyaman saat makan. ',\n",
      "  'Pada saat pemberian makan, bayi memang sebaiknya diberikan air putih '\n",
      "  'sebagai pelarut makanan agar mudah dikonsumsi. '],\n",
      " ['Namun sebenarnya pemberian air putih tidak disarankan tidak juga dilarang. ',\n",
      "  'Pada dasarnya, jangan sampai air putih tersebut menggantikan ASI karena air '\n",
      "  'putih tidak memiliki nilai kalori. ',\n",
      "  'Jadi tidak ada jumlah khusus tertentu berapa banyak anak harus minum air '\n",
      "  'putih dalam sehari, yang terpenting konsumsi air tidak boleh melebihi 120cc '\n",
      "  'dalam sehari. ',\n",
      "  \"Semoga informasi ini bermanfaat :').  \",\n",
      "  'Jika anda merasakan ada hal yang ingin ditanyakan lebih jauh dan lebih '\n",
      "  'jelas lagi mengenai pemberian MPASI anak, sebaiknya diskusikan permasalahan '\n",
      "  'ini dengan dokter spesialis anak.  ',\n",
      "  'dr. ',\n",
      "  'Lia N. ',\n",
      "  'Amalina. ']]\n"
     ]
    }
   ],
   "source": [
    "from textsplit.tools import get_penalty, get_segments\n",
    "from textsplit.algorithm import split_optimal, split_greedy, get_total\n",
    "from pprint import pprint\n",
    "\n",
    "segment_len = 4\n",
    "penalty = get_penalty([sentence_vec], segment_len)\n",
    "\n",
    "optimal_segmentation = split_optimal(sentence_vec, penalty, seg_limit=250)\n",
    "segmented_text = get_segments(sentenced_text, optimal_segmentation)\n",
    "\n",
    "print(f\"{len(sentenced_text)} sentences, {len(segmented_text)} segments, avg. {len(sentenced_text) // len(segmented_text)} sentences per segment\")\n",
    "pprint(segmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67700e6e-fdf9-4db0-a018-2bb894d49750",
   "metadata": {},
   "source": [
    "## Segmentasi menggunakan algoritma Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db2e79-4b50-46ca-867a-69fb1b1bd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_segmentation = split_greedy(sentence_vec, max_splits=len(optimal_segmentation.splits))\n",
    "greedy_segmented_text = get_segments(sentenced_text, greedy_segmentation)\n",
    "\n",
    "print(f\"{len(sentenced_text)} sentences, {len(greedy_segmented_text)} segments, avg. {len(sentenced_text) // len(greedy_segmented_text)} sentences per segment\")\n",
    "pprint(greedy_segmented_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effdbb4c",
   "metadata": {},
   "source": [
    "# UTS Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5beb0fcd-f17d-4a2d-96a3-d0dc2c6e202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/intfloat/uts\n",
    "\n",
    "import uts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2457d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "document = sentenced_text\n",
    "model = uts.C99(window=2)\n",
    "boundary = model.segment(document)\n",
    "# output: [1, 0, 1, 0]\n",
    "print(boundary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5133cb",
   "metadata": {},
   "source": [
    "# Centre Borelli - ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2caf107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import ruptures as rpt  # our package\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from ruptures.base import BaseCost\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a23f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/selomitazhafiirah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORD_SET = set(\n",
    "    stopwords.words(\"english\")\n",
    ")  # set of stopwords of the English language\n",
    "#stopword nya englsih, datamya indonesia\n",
    "PUNCTUATION_SET = set(\"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d112c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(list_of_sentences: list) -> list:\n",
    "    \"\"\"Preprocess each sentence (remove punctuation, stopwords, then stemming.)\"\"\"\n",
    "    transformed = list()\n",
    "    for sentence in list_of_sentences:\n",
    "        ps = PorterStemmer()\n",
    "        list_of_words = regexp_tokenize(text=sentence.lower(), pattern=\"\\w+\")\n",
    "        list_of_words = [\n",
    "            ps.stem(word) for word in list_of_words if word not in STOPWORD_SET\n",
    "        ]\n",
    "        transformed.append(\" \".join(list_of_words))\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e484f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_square_on_ax(start, end, ax, linewidth=0.8):\n",
    "    \"\"\"Draw a square on the given ax object.\"\"\"\n",
    "    ax.vlines(\n",
    "        x=[start - 0.5, end - 0.5],\n",
    "        ymin=start - 0.5,\n",
    "        ymax=end - 0.5,\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "    ax.hlines(\n",
    "        y=[start - 0.5, end - 0.5],\n",
    "        xmin=start - 0.5,\n",
    "        xmax=end - 0.5,\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ec4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (line_number, sentence) in enumerate(sentence_example.split('.')):\n",
    "    sentence = sentence.strip(\"\\n\")\n",
    "    print(f\"{line_number:>2}: {sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbd4da7a",
   "metadata": {},
   "source": [
    "## id-sentence-segmenter\n",
    "from https://github.com/yudanta/id-sentence-segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ecee30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'idsentsegmenter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39midsentsegmenter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentence_segmentation\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceSegmentation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'idsentsegmenter'"
     ]
    }
   ],
   "source": [
    "from idsentsegmenter.sentence_segmentation import SentenceSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence segmenter instance from SentenceSegmentation class\n",
    "sentence_segmenter = SentenceSegmentation()\n",
    "\n",
    "# parse text to sentences \n",
    "sentences = sentence_segmenter.get_sentences(news_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('3.10.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "965adcd639ca764653955ffc8e8cde92f20b9069d5d62fc9b3c038bcd069a914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
